Add a new scenario called mixed_scenario that runs two jobs concurrently inside one SU and produces comparable per-job and per-step metrics under different routing modes. Use the existing SU topology builder and node/rack mapping used by ai_factory_su_scenario (or the closest equivalent in the repo), and implement this scenario under ai_factory_simulation/scenarios/ as mixed_scenario.py (and the workload builder under ai_factory_simulation/workloads/mixed_scenario.py if applicable). The SU is the same topology we have been using: 32 nodes total, arranged as 8 racks with 4 nodes per rack, each rack has one leaf/ToR, leaves connect to 4 spines with multiple equal-cost paths. The scenario must allocate exactly half of the nodes to job A (Workload 2, tensor-parallel-heavy pattern) and the other half to job B (Workload 3, pipeline-parallel + data-parallel mixed pattern). Both jobs start at the same simulation time and run for the same number of steps so that their interaction is stable and measurable.

Use a deterministic node split based on the existing node ordering so the result is reproducible. If the code has a canonical list of node IDs (for example in topology creation), take that list and assign the first 16 nodes to job A and the remaining 16 nodes to job B. If the project prefers rack-aware allocation for realism, keep it deterministic but balanced across racks: for each rack, assign the first two nodes in that rack to job A and the other two nodes to job B, resulting in each job having 16 nodes evenly spread across all racks. Implement both options as a parameter named allocation_mode with default rack_balanced, and allow switching to contiguous for debugging.

Job A must represent a TP-heavy workload. Implement it as a per-step program that alternates compute and small collectives many times, then optionally a small DP sync near the end of the step. Do not implement GPU-rank-level detail; operate at the node level using your existing Collective templates that expand to flows. For each step of job A, schedule a compute phase of 5 ms, then schedule a sequence of M communication micro-phases that emulate TP collectives. Choose M to be 32 by default (make it configurable), and for each micro-phase do a collective over the entire job A node group using either all-reduce or all-gather (whichever is already implemented and easiest). The payload for each micro-phase should be small enough to avoid dominating runtime but large enough to create queue interactions, for example 1 MB per node per micro-phase as a starting point (configurable). Between micro-phases, insert a short compute phase of 0.5 ms to create natural gaps that enable flowlet switching to form new flowlets; this gap is important for differentiating ECMP vs flowlet vs adaptive shortest-queue. After the M micro-phases, add a final communication phase that represents DP synchronization (or optimizer shard sync) across job A’s group, using reduce-scatter plus all-gather if those templates exist, otherwise a single all-reduce. Make the final sync larger, for example 16 MB per node, so that step tail latency matters. End the step with a 2 ms compute phase. Ensure that each comm micro-phase is barriered: the step cannot advance to the next micro-phase until all flows emitted by the current collective have completed, and similarly for the final sync. The goal is to make the job sensitive to tail and jitter across many small collectives while still having one larger sync to anchor the step time.

Job B must represent PP+DP. Model 4 pipeline stages within job B’s 16 nodes. Implement stage assignment deterministically so it is reproducible and can be toggled between topology-aware and topology-unaware placement. In topology-aware mode, place adjacent stages close by grouping nodes by rack: for example assign stage 0 to nodes in racks 0–1, stage 1 to racks 2–3, stage 2 to racks 4–5, stage 3 to racks 6–7, with an equal number of nodes per stage (4 nodes per stage). In topology-unaware mode, randomly permute the 16 nodes with a fixed seed and then assign the first 4 to stage 0, next 4 to stage 1, etc. Expose this as a parameter stage_placement_mode with default topology_aware, and seed must be passed explicitly or derived from a scenario seed.

For each step of job B, simulate microbatch pipeline traffic as streams between adjacent stages plus a DP sync at the end. Do not create per-packet streaming; generate flow bursts that approximate sustained stage-to-stage transfers for a fixed window. For example, represent the forward activation sends as a set of flows from each node in stage k to one mapped partner node in stage k+1, repeated for a microbatch count. Implement a simple partner mapping that is deterministic and one-to-one: node i in stage k maps to node i (by index within the stage) in stage k+1. Use a microbatch_count parameter with default 8. For each microbatch, schedule flows stage0->stage1, stage1->stage2, stage2->stage3 with size activation_bytes_per_microbatch per sender, default 2 MB, and insert a small inter-microbatch gap of 50–100 microseconds to create bursts rather than a single monolithic transfer; keep this configurable. After finishing forward transfers, schedule backward gradient transfers in reverse direction stage3->stage2, stage2->stage1, stage1->stage0 with size grad_bytes_per_microbatch per sender, default 2 MB, with the same microbatch_count and gap logic. After these pipeline transfers complete, schedule a DP synchronization across all 16 nodes in job B, using reduce-scatter plus all-gather if available, otherwise all-reduce. The DP sync payload should be moderate, default 32 MB per node, so it competes with job A’s comm and creates contention on leaf-spine links. End the step with a 2–5 ms compute phase. As with job A, enforce barrier semantics between the major blocks: do not start backward until forward flows have completed, do not start DP sync until all backward flows are done. Inside forward and backward, microbatches can overlap if your runner supports it, but keep it sequential at first for simplicity and determinism.

Both jobs must share the same network and run concurrently. Implement them using the same JobRunner mechanism, but ensure the runners can progress independently and interleave events naturally in the DES. If the current design assumes a single runner owns the DES loop, extend it so multiple jobs can be registered and scheduled without blocking each other. All flow completion callbacks must be routed back to the correct job/phase join object. Ensure that the tag/metadata on each Flow includes job_id and a concise phase tag so later analysis can separate traffic types, for example jobA_tp_micro, jobA_dp_sync, jobB_pp_fwd, jobB_pp_bwd, jobB_dp_sync.

Routing mode must be selectable at scenario construction time. Use the existing routing hooks and implement a scenario parameter routing_mode with values ecmp, flowlet, adaptive_shortest_queue. The scenario should run once per mode when invoked from tests or from a script, or at least make it easy to run all modes via a loop. For flowlet switching, ensure the flowlet-idle gap threshold is configurable and set a default that is comparable to the micro-gaps inserted in job A and job B so flowlets actually form. For adaptive shortest-queue, ensure the queue length metric used is the egress port queue length on the current node or switch output port, whichever is already implemented; do not invent a new queue metric. If adaptive routing expects per-packet decisions but you are at flow-level, apply the adaptive decision at flow start (or at flowlet boundaries if you already have flowlets) so it is consistent.

Collect metrics per job and overall. For each job, record step start time and step end time and compute step duration. Also record durations of major blocks: for job A, total time spent in TP micro-phases versus DP sync; for job B, forward pipeline time, backward pipeline time, and DP sync time. Compute p50, p95, p99 of step durations for each job separately and also overall throughput proxy such as steps completed per simulated second for each job. Log these metrics using the project’s logging utilities and save them in the same format as other scenarios so your visualization code can pick them up. Also record a timeline of queue occupancy on at least one representative bottleneck link (for example a specific leaf-to-spine uplink) if that instrumentation already exists, because it helps explain why routing modes differ; if instrumentation does not exist, skip adding it.

Keep the simulation cheap by default. Provide scenario parameters to reduce load while preserving behavior: steps default 400, jobA_micro_collectives M default 32, microbatch_count default 8, payload sizes as above. Provide a scale factor named traffic_scale that multiplies all payload sizes, default 1.0, and allow quick runs with 0.25 or 0.1. If packetization is still expensive, ensure the flow-to-packet adapter uses the project’s coarsening knob (larger “packet payload” or fewer packets per flow), and expose it as scenario config without changing the network model semantics.

Add a minimal unit test that instantiates mixed_scenario in a tiny configuration and asserts completion and determinism. Use 2 racks, 1 node per rack per job (4 nodes total) for the test, reduce steps to 5, reduce payload sizes accordingly, and assert both jobs complete all steps and that the per-job step-duration arrays have the expected lengths. Also add a determinism test that with fixed seed the expanded flows for the first step have a stable signature (hash the tuple of src,dst,size,start,job_id,tag).
